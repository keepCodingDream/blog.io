---
layout: post
title: 神经网络训练的理论推导
category: 深度学习
keywords: 深度学习,FP,BP
---

> 学习一门新的知识，我更喜欢追根溯源，理解它背后的原理。

## 基础知识

#### 1.矩阵计算
神经网络无论是训练还是预测都是基于矩阵运算，这样能大大降低训练、预测的时间。所以，关于矩阵计算相关的知识是必不可少的，如果在阅读过程中关于这部分有任何问题，都可以到[矩阵运算及其规则](http://www2.edu-edu.com.cn/lesson_crs78/self/j_0022/soft/ch0605.html)以及[Numpy官方文档](https://docs.scipy.org/doc/numpy-1.10.1/index.html) 查阅。

#### 2.梯度下降
神经网络训练的过程中是运用梯度下降的方式优化每个隐藏层节点的权重，所以，如果想要完全理解训练的整个过程，请先完成对[梯度下降](http://www.cnblogs.com/fxjwind/p/3626173.html)的理解。

## 神经网络的基本结构
![](https://raw.githubusercontent.com/keepCodingDream/blog.io/master/assets/img/neural/1.1.png)
 
上图是一个非常简单的神经网络模型，其中每个圆点我们称作 `神经元` 。

图中最左边一列的X1、X2被称作 `输入层` 也就是最基本的输入数据。

第二、第三列被称作 `隐藏层` 。之所以被称作隐藏层，就是因为他们在整个模型的计算过程中虽然起着非常重要的作用，但我们并不能看到他们的结果，或者说他们的结果对我们来说是没有多少意义的。理论是，隐藏层是可以无限多的。

最后一列只有一个原点，被称作 `输出层` (当然输出层也可以是多个输出节点)。顾名思义，他就是我们模型的计算结果。


## 神经网络的基本运算

大家可以注意到，每一层和下一层之间都会有一条直线连接，我们可以把直线理解为数据的输入，神经元理解为计算单元。所以，每一层的神经元计算都是利用上一层的计算结果，计算完成以后再将结果传递给下一层，直到最终结果的输出。

神经元的运算包括两步，第一步是一个简单的线性计算，我们可以先用最简单的线性运算: \\(y = k*x+b\\) 去表达。

线性计算完成以后，我们还需要对计算的结果做一次非线性的转换，也就是第二步运算，这种转换被称为 `激活函数` 。我们暂时使用最简单的非线性函数[sigmoid](https://baike.baidu.com/item/Sigmoid%E5%87%BD%E6%95%B0/7981407?fr=aladdin)去作为激活函数。

**正是因为激活函数的存在，才让神经网络可以作为非线性可分数据的分类器。** 至于为什么需要激活函数，以及激活函数为何一定得是非线性的，我们会在以后介绍。

所以每个神经的计算可以概括为:

$$
  Z=W*X+b
$$

$$
A=sigmoid(Z)
$$


## Forword Propagate (正向传播)

有了上面两步的介绍，就可以正式开始介绍整个神经网络计算的过程了。

因为第一列(第0层)是输入层，所以我们不做运算，我们从第1层开始，第一层的运算直接使用第0层的输出，所以我们可以表达为:

$$
  Z^1=W^1*X+b^1
$$

$$
A^1=sigmoid(Z^1)
$$

上面公式中的上标1表示是第一层的运算参数。每一层的W和b都需要在训练之前初始化好，然后在训练的过程中不断更正，具体会在下面 `反向传播` 中介绍。

以此类推，第二层的计算就是:

$$
  Z^2=W^2*A^1+b^2
$$

$$
	A^2=sigmoid(Z^2)
$$

所以，总结下来，整个计算的公式可以概括为:

$$
  Z^n=W^n*A^{n-1}+b^n
$$

$$
	A^n=sigmoid(Z^n)
$$

$$
	其中A^0=X
$$

